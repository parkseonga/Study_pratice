{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuring engeering \n",
    "1. 푸리에 급수이용   \n",
    "[데이콘 대회 참고](https://dacon.io/competitions/official/235608/codeshare/1130?page=1&dtype=recent&ptype=pub)  \n",
    "[푸리에 변환 설명 참고](https://www.youtube.com/watch?v=spUNpyF58BY&feature=youtu.be)   \n",
    "\n",
    "\n",
    "    - 푸리에 급수: 주기 함수를 삼각함수의 가중치로 분해한 급수\n",
    "    - 푸리에 변환: 입력 신호를 다양한 주파수를 갖는 주기함수들의 합으로 분해하여 표현하는 방법 \n",
    "    \n",
    "![image](https://user-images.githubusercontent.com/33725048/85993286-0ea3c880-ba31-11ea-9dab-418597e6b786.png)\n",
    "\n",
    "\n",
    "[주파수 영역 이해 참고 사진](https://en.wikipedia.org/wiki/Fourier_transform)\n",
    "    - 푸리에 스펙트럼: 주파수 성분이 원 신호에 얼마나 강하게 포함되어 있는가(주파수 성분의 강도)\n",
    "\n",
    "\n",
    "\n",
    "2. 통계적 특질 사용\n",
    "    - 신호를 분석할 때 통계 특질, 피크 특질, 주파수 특질 등을 통해 특질을 추출해낼 수 있는데 그 중 통계적  특질을 이용 \n",
    "    - [신호데이터 분석을 할 때 특질을 공부하며 작성했던 다소 미흡한 블로그 링크입니다.(R Programming)](https://ssung-22.tistory.com/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import missingno as msno\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgbm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['layer_1', 'layer_2', 'layer_3', 'layer_4', '0', '1', '2', '3', '4',\n",
      "       '5',\n",
      "       ...\n",
      "       '216', '217', '218', '219', '220', '221', '222', '223', '224', '225'],\n",
      "      dtype='object', length=230)\n",
      "Index(['id', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
      "       ...\n",
      "       '216', '217', '218', '219', '220', '221', '222', '223', '224', '225'],\n",
      "      dtype='object', length=227)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#독립변수와 종속변수를 분리합니다.\n",
    "train_X = train.iloc[:,4:]\n",
    "train_Y = train.iloc[:,0:4]\n",
    "test_X = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 푸리에 급수 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "feature_col = list(train_X)\n",
    "\n",
    "alpha_real = train_X[feature_col]\n",
    "alpha_imag = train_X[feature_col]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in feature_col:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "feature_col = list(test_X)\n",
    "\n",
    "alpha_real = test_X[feature_col]\n",
    "alpha_imag = test_X[feature_col]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in feature_col:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns = real_part\n",
    "alpha_imag.columns = imag_part\n",
    "alpha_test = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "test_X = pd.concat((test_X, alpha_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.concat((train_X, alpha), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# alpha.to_pickle('DFT.pickle')\n",
    "pickle.dump(alpha, open('DFT.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X, tst_X, trn_y, tst_y = train_test_split(train_X, train_Y, test_size=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 설정\n",
    "train_set = lgbm.Dataset(trn_X, trn_y)\n",
    "valid_set = lgbm.Dataset(tst_X, tst_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 기초 파라미터 설정\n",
    "lgb_param = {'objective': 'regression', \n",
    "            'n_estimators': 500,\n",
    "            'drop_rate': 0.8, \n",
    "            'skip_drop': 0.8, \n",
    "            'learning_rate' : 0.5,\n",
    "            'max_depth' : 6,\n",
    "            'random_state' : 42,\n",
    "            'metric' : 'l1',\n",
    "            'colsample_bytree' : 0.7,\n",
    "            'subsample' : 0.7,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_X = trn_X.astype('float32')\n",
    "tst_X = tst_X.astype('float32')\n",
    "\n",
    "trn_X.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for col in trn_y.columns:\n",
    "    train_set = lgbm.Dataset(trn_X, trn_y[col])\n",
    "    valid_set = lgbm.Dataset(tst_X, tst_y[col])\n",
    "    model = lgbm.train(lgb_param, train_set=train_set, valid_sets=valid_set,\n",
    "                        num_boost_round = 1000, verbose_eval=10)\n",
    "    models[col] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_Y.columns:\n",
    "    pred = models[col].predict(test_X)\n",
    "    sample_sub[col] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv('lgbm_baseline.csv') # 33.9점 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 통계적 특질 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수를 고려하여 신호의 크기를 나타내는 값 \n",
    "import numpy as np\n",
    "from scipy.stats import kurtosis, iqr\n",
    "\n",
    "def rms(x):\n",
    "    return np.sqrt(np.mean(x**2))\n",
    "\n",
    "def rss(x):\n",
    "    return rms(x)*len(x)\n",
    "\n",
    "# 왜도 \n",
    "\n",
    "def skewness(x):\n",
    "    return (sum((x-np.mean(x))**3)/len(x))/(sum((x-np.mean(x))**2)/len(x))**(3/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_list = ['mean', 'min', 'max', 'std', skewness, rss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X2 = train_X.aggregate(function_list,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X2 = test_X.aggregate(function_list,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "alpha = pickle.load(open('DFT.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trn_X, tst_X, trn_y, tst_y = train_test_split(train_X2, train_Y, test_size=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for col in trn_y.columns:\n",
    "    train_set = lgbm.Dataset(trn_X, trn_y[col])\n",
    "    valid_set = lgbm.Dataset(tst_X, tst_y[col])\n",
    "    model = lgbm.train(lgb_param, train_set=train_set, valid_sets=valid_set,\n",
    "                        num_boost_round = 1000, verbose_eval=10)\n",
    "    models[col] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_Y.columns:\n",
    "    pred = models[col].predict(test_X2)\n",
    "    sample_sub[col] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv('lgbm_baseline_summary0627.csv')  # 63점 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "통계적 특질만 적용해서하면 점수가 매우 낮음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "푸리에급수를 적용한 것과 통계적 특질 적용한 것 모두 사용하면 메모리 에러 발생."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
